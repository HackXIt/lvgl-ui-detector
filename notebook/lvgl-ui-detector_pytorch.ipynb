{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: terminaltables\n",
      "Successfully installed terminaltables-3.1.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The notebook is intended to be run inside a tensorflow docker container\n",
    "# To run without container, run: %pip install torch torchvision\n",
    "# Additional packages\n",
    "# %pip install matplotlib numpy pillow opencv-python-headless torchvision torch>=1.0 tensorflow tensorboard terminaltables tqdm\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pillow\n",
    "%pip install opencv-python-headless\n",
    "# %pip install torchvision\n",
    "# %pip install torch>=1.0\n",
    "# %pip install tensorflow\n",
    "# %pip install tensorboard\n",
    "%pip install terminaltables\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "RGB_MAX = 255.0\n",
    "BASE_DIR = '/app/project/' # Mounted project directory inside container\n",
    "IMG_SIZE = 250 # Always square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Paths for data\n",
    "weights_dir = os.path.join(BASE_DIR, 'weights')\n",
    "rico_datasets = {\"rico\": os.path.join(weights_dir, 'rico'), \n",
    "                 \"rico2k\": os.path.join(weights_dir, 'rico2k'), \n",
    "                 \"rico5box\": os.path.join(weights_dir, 'rico5box'), \n",
    "                 \"rico10k\": os.path.join(weights_dir, 'rico10k'), \n",
    "                 \"ricotext\": os.path.join(weights_dir, 'ricotext')}\n",
    "base_data_dir = os.path.join(BASE_DIR, 'tmp', 'data')\n",
    "base_train_dir = os.path.join(base_data_dir, 'train')\n",
    "base_test_dir = os.path.join(base_data_dir, 'test')\n",
    "extensions = {\"image\": '.jpg', \"label\": '.txt'}\n",
    "classes = {\n",
    "    \"lv_btn\": {\"name\": \"button\", \"index\": 0},\n",
    "    \"lv_checkbox\": {\"name\": \"checkbox\", \"index\": 1},\n",
    "    \"lv_label\": {\"name\": \"label\", \"index\": 2},\n",
    "    \"lv_slider\": {\"name\": \"slider\", \"index\": 3},\n",
    "    \"lv_switch\": {\"name\": \"switch\", \"index\": 4}\n",
    "}\n",
    "class_names = [classes[key][\"name\"] for key in classes.keys()]\n",
    "widget_names = [key for key in classes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/project/tmp/data/train/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_train_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m label_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_train_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mYoloDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mYoloDataset.__init__\u001b[0;34m(self, image_folder, label_folder, S, B, C, transform)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_folder, label_folder, S\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m26\u001b[39m, \u001b[38;5;241m52\u001b[39m], B\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_folder, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(label_folder)]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m416\u001b[39m  \u001b[38;5;66;03m# YOLOv3 uses 416x416 images\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/project/tmp/data/train/images'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, S=[13, 26, 52], B=3, C=80, transform=None):\n",
    "        self.image_files = [os.path.join(image_folder, x) for x in os.listdir(image_folder)]\n",
    "        self.label_files = [os.path.join(label_folder, x) for x in os.listdir(label_folder)]\n",
    "        self.image_size = 416  # YOLOv3 uses 416x416 images\n",
    "        self.S = S  # List of scales\n",
    "        self.B = B  # Number of bounding boxes\n",
    "        self.C = C  # Number of classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load and transform the image\n",
    "        image_file = self.image_files[index]\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Initialize label tensors for each scale\n",
    "        label_tensor_list = [torch.zeros((self.C + 5) * self.B, s, s) for s in self.S]\n",
    "\n",
    "        # Get correct label file for the image \n",
    "        # (rpartition will split from end of string to character: 0=path, 1=basename)\n",
    "        label_index = self.label_files.index(image_file.rpartition('/')[1]) # Use image basename to find label\n",
    "        # Load label\n",
    "        label_file = self.label_files[label_index]\n",
    "        boxes = []\n",
    "        with open(label_file) as f:\n",
    "            for line in f.readlines(): # Each line is a box\n",
    "                class_id, x_center, y_center, width, height = [\n",
    "                    float(x) for x in line.replace('\\n', '').split()\n",
    "                ]\n",
    "                boxes.append([class_id, x_center, y_center, width, height, 1])\n",
    "\n",
    "        # Fill the label tensors\n",
    "        for box in boxes:\n",
    "            class_id, x_center, y_center, width, height, confidence = box\n",
    "            # We assume that the annotations were normalized by the width and height of the image.\n",
    "            # i.e., x_center and width are divided by the width of the image\n",
    "            # and similarly for y_center and height.\n",
    "\n",
    "            # Assign the box to the tensor corresponding to the scale\n",
    "            for scale_idx, s in enumerate(self.S):\n",
    "                i, j = int(s * y_center), int(s * x_center)  # Which grid cell\n",
    "                anchor_on_scale = scale_idx  # Which anchor (here we're just using the index)\n",
    "                \n",
    "                # Locate the cell responsible and assign the bounding box\n",
    "                label_tensor = label_tensor_list[scale_idx]\n",
    "                label_tensor[..., i, j] = torch.tensor(\n",
    "                    [x_center, y_center, width, height, confidence] + [0] * self.C\n",
    "                )\n",
    "                label_tensor[class_id, i, j] = 1\n",
    "\n",
    "        return image, label_tensor_list\n",
    "\n",
    "# Usage\n",
    "image_dir = os.path.join(base_train_dir, 'images')\n",
    "label_dir = os.path.join(base_train_dir, 'labels')\n",
    "dataset = YoloDataset(image_dir, label_dir)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `S` is a list of the sizes of the feature maps at different scales.\n",
    "- `B` is the number of anchors used.\n",
    "- `C` is the number of classes in the dataset.\n",
    "- The label tensors are initialized as zero tensors for each feature map scale.\n",
    "- For each object in the image, the correct cell in each scale's feature map is located, and the bounding box and class label are placed in the corresponding position in the label tensor.\n",
    "- The bounding boxes are assumed to be normalized, with coordinates as fractions of the image dimensions. You may need to adjust this if your labels are in a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "from models import ModelYOLOv3  # This should match the class name of the YOLOv3 model in your models.py\n",
    "\n",
    "# Initialize the YOLOv3 model\n",
    "num_classes = 80  # Replace with the number of classes in your dataset\n",
    "model = ModelYOLOv3(num_classes).to(device)  # Make sure to pass the correct number of classes to the model\n",
    "\n",
    "# Load pre-trained weights (if available)\n",
    "model.load_state_dict(torch.load(rico_datasets[\"rico\"], map_location=device))\n",
    "\n",
    "# Continue with setting up your dataset and data loaders\n",
    "image_folder = 'path/to/images'\n",
    "label_folder = 'path/to/labels'\n",
    "\n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),  # Resize to input size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Instantiate your custom dataset\n",
    "dataset = YoloDataset(image_folder, label_folder, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Define your optimizer\n",
    "for epoch in range(num_epochs):\n",
    "    for imgs, targets in loader:\n",
    "        imgs, targets = imgs.to(device), [target.to(device) for target in targets]  # Move to the appropriate device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(imgs)  # Forward pass\n",
    "        loss = compute_loss(output, targets)  # You'll need to define compute_loss according to YOLOv3 loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# compute_loss is a function you will need to define. It should calculate the loss for YOLOv3.\n",
    "# This will involve calculating the objectness loss, the class prediction loss,\n",
    "# and the bounding box regression loss. You will likely need to iterate through\n",
    "# each of the three scales that YOLOv3 outputs and calculate the loss for each,\n",
    "# then sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "from loss import YoloLoss  # This should be the path to your YOLOv3 loss function implementation\n",
    "\n",
    "criterion = YoloLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
