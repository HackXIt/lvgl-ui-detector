{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.2.0)\n",
      "Collecting torch==2.1.2\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m475.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (4.8.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (2.18.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchvision) (12.3.101)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.1\n",
      "    Uninstalling torch-2.1.1:\n",
      "      Successfully uninstalled torch-2.1.1\n",
      "Successfully installed torch-2.1.2 torchvision-0.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (3.1.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The notebook is intended to be run inside a tensorflow docker container\n",
    "# To run without container, run: %pip install torch torchvision\n",
    "# Additional packages\n",
    "# %pip install matplotlib numpy pillow opencv-python-headless torchvision torch>=1.0 tensorflow tensorboard terminaltables tqdm\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pillow\n",
    "%pip install opencv-python-headless\n",
    "%pip install torchvision\n",
    "# %pip install torch>=1.0\n",
    "# %pip install tensorflow\n",
    "# %pip install tensorboard\n",
    "%pip install terminaltables\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "RGB_MAX = 255.0\n",
    "BASE_DIR = '/app/project/' # Mounted project directory inside container\n",
    "IMG_SIZE = 416 # Always square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Paths for data\n",
    "baseline_dir = os.path.join(BASE_DIR, 'baseline', 'PyTorch-YOLOv3')\n",
    "config_dir = os.path.join(baseline_dir, 'config')\n",
    "yolov3_configs = {\"yolov3\": os.path.join(config_dir, 'yolov3.cfg'),\n",
    "                  \"yolov3-rico\": os.path.join(config_dir, 'yolov3-rico.cfg'),\n",
    "                  \"yolov3-rico2k\": os.path.join(config_dir, 'yolov3-rico2k.cfg'),\n",
    "                  \"yolov3-rico5box\": os.path.join(config_dir, 'yolov3-rico5box.cfg'),\n",
    "                  \"yolov3-rico10k\": os.path.join(config_dir, 'yolov3-rico10k.cfg'),\n",
    "                  \"yolov3-ricotext\": os.path.join(config_dir, 'yolov3-ricotext.cfg'),\n",
    "                  \"yolov3-custom\": os.path.join(config_dir, 'yolov3-custom.cfg')}\n",
    "data\n",
    "weights_dir = os.path.join(BASE_DIR, 'weights')\n",
    "rico_weights = {\"rico\": os.path.join(weights_dir, 'rico'), \n",
    "                 \"rico2k\": os.path.join(weights_dir, 'rico2k'), \n",
    "                 \"rico5box\": os.path.join(weights_dir, 'rico5box'), \n",
    "                 \"rico10k\": os.path.join(weights_dir, 'rico10k'), \n",
    "                 \"ricotext\": os.path.join(weights_dir, 'ricotext')}\n",
    "base_data_dir = os.path.join(BASE_DIR, 'tmp', 'data')\n",
    "base_train_dir = os.path.join(base_data_dir, 'train')\n",
    "base_test_dir = os.path.join(base_data_dir, 'test')\n",
    "extensions = {\"image\": '.jpg', \"label\": '.txt'}\n",
    "classes = {\n",
    "    \"lv_btn\": {\"name\": \"button\", \"index\": 0},\n",
    "    \"lv_checkbox\": {\"name\": \"checkbox\", \"index\": 1},\n",
    "    \"lv_label\": {\"name\": \"label\", \"index\": 2},\n",
    "    \"lv_slider\": {\"name\": \"slider\", \"index\": 3},\n",
    "    \"lv_switch\": {\"name\": \"switch\", \"index\": 4}\n",
    "}\n",
    "class_names = [classes[key][\"name\"] for key in classes.keys()]\n",
    "widget_names = [key for key in classes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, S=[13, 26, 52], B=3, C=80, transform=None):\n",
    "        self.image_files = [os.path.join(image_folder, x) for x in os.listdir(image_folder)]\n",
    "        self.label_files = [os.path.join(label_folder, x) for x in os.listdir(label_folder)]\n",
    "        self.image_size = 416  # YOLOv3 uses 416x416 images\n",
    "        self.S = S  # List of scales\n",
    "        self.B = B  # Number of bounding boxes\n",
    "        self.C = C  # Number of classes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load and transform the image\n",
    "        image_file = self.image_files[index]\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Initialize label tensors for each scale\n",
    "        label_tensor_list = [torch.zeros((self.C + 5) * self.B, s, s) for s in self.S]\n",
    "\n",
    "        # Get correct label file for the image \n",
    "        # (rpartition will split from end of string to character: 0=path, 1=basename)\n",
    "        label_index = self.label_files.index(image_file.rpartition('/')[1]) # Use image basename to find label\n",
    "        # Load label\n",
    "        label_file = self.label_files[label_index]\n",
    "        boxes = []\n",
    "        with open(label_file) as f:\n",
    "            for line in f.readlines(): # Each line is a box\n",
    "                class_id, x_center, y_center, width, height = [\n",
    "                    float(x) for x in line.replace('\\n', '').split()\n",
    "                ]\n",
    "                boxes.append([class_id, x_center, y_center, width, height, 1])\n",
    "\n",
    "        # Fill the label tensors\n",
    "        for box in boxes:\n",
    "            class_id, x_center, y_center, width, height, confidence = box\n",
    "            # We assume that the annotations were normalized by the width and height of the image.\n",
    "            # i.e., x_center and width are divided by the width of the image\n",
    "            # and similarly for y_center and height.\n",
    "\n",
    "            # Assign the box to the tensor corresponding to the scale\n",
    "            for scale_idx, s in enumerate(self.S):\n",
    "                i, j = int(s * y_center), int(s * x_center)  # Which grid cell\n",
    "                anchor_on_scale = scale_idx  # Which anchor (here we're just using the index)\n",
    "                \n",
    "                # Locate the cell responsible and assign the bounding box\n",
    "                label_tensor = label_tensor_list[scale_idx]\n",
    "                label_tensor[..., i, j] = torch.tensor(\n",
    "                    [x_center, y_center, width, height, confidence] + [0] * self.C\n",
    "                )\n",
    "                label_tensor[class_id, i, j] = 1\n",
    "\n",
    "        return image, label_tensor_list\n",
    "\n",
    "# Usage\n",
    "image_dir = os.path.join(base_train_dir, 'images')\n",
    "label_dir = os.path.join(base_train_dir, 'labels')\n",
    "dataset = YoloDataset(image_dir, label_dir)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `S` is a list of the sizes of the feature maps at different scales.\n",
    "- `B` is the number of anchors used.\n",
    "- `C` is the number of classes in the dataset.\n",
    "- The label tensors are initialized as zero tensors for each feature map scale.\n",
    "- For each object in the image, the correct cell in each scale's feature map is located, and the bounding box and class label are placed in the corresponding position in the label tensor.\n",
    "- The bounding boxes are assumed to be normalized, with coordinates as fractions of the image dimensions. You may need to adjust this if your labels are in a different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from baseline\n",
    "import sys\n",
    "sys.path.append(os.path.join(BASE_DIR, 'baseline', 'PyTorch-YOLOv3'))\n",
    "from models import Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't concat NoneType to bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the YOLOv3 model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Replace with the number of classes in your dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDarknet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Make sure to pass the correct number of classes to the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load pre-trained weights (if available)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(rico_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrico\u001b[39m\u001b[38;5;124m\"\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "File \u001b[0;32m/app/project/baseline/PyTorch-YOLOv3/models.py:239\u001b[0m, in \u001b[0;36mDarknet.__init__\u001b[0;34m(self, config_path, img_size)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_path, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m416\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Darknet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_defs \u001b[38;5;241m=\u001b[39m \u001b[43mparse_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list \u001b[38;5;241m=\u001b[39m create_modules(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_defs)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo_layers \u001b[38;5;241m=\u001b[39m [layer[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m/app/project/baseline/PyTorch-YOLOv3/utils/parse_config.py:6\u001b[0m, in \u001b[0;36mparse_model_config\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m lines \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      8\u001b[0m lines \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mlstrip() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m lines] \u001b[38;5;66;03m# get rid of fringe whitespaces\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:321\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't concat NoneType to bytes"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Initialize the YOLOv3 model\n",
    "num_classes = 4  # Replace with the number of classes in your dataset\n",
    "model = Darknet(num_classes).to(device)  # Make sure to pass the correct number of classes to the model\n",
    "\n",
    "# Load pre-trained weights (if available)\n",
    "model.load_state_dict(torch.load(rico_datasets[\"rico\"], map_location=device))\n",
    "\n",
    "# Continue with setting up your dataset and data loaders\n",
    "image_folder = os.path.join(base_train_dir, 'images')\n",
    "label_folder = os.path.join(base_train_dir, 'labels')\n",
    "\n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize to input size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Instantiate your custom dataset\n",
    "dataset = YoloDataset(image_folder, label_folder, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "from loss import YoloLoss  # This should be the path to your YOLOv3 loss function implementation\n",
    "\n",
    "criterion = YoloLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50 # Set the number of epochs to train for\n",
    "for epoch in range(num_epochs):\n",
    "    for imgs, targets in loader:\n",
    "        imgs, targets = imgs.to(device), [target.to(device) for target in targets]  # Move to the appropriate device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(imgs)  # Forward pass\n",
    "        loss = compute_loss(output, targets)  # You'll need to define compute_loss according to YOLOv3 loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# compute_loss is a function you will need to define. It should calculate the loss for YOLOv3.\n",
    "# This will involve calculating the objectness loss, the class prediction loss,\n",
    "# and the bounding box regression loss. You will likely need to iterate through\n",
    "# each of the three scales that YOLOv3 outputs and calculate the loss for each,\n",
    "# then sum them up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
